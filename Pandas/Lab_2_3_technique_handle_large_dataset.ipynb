{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drshahizan/Python_Tutorial/blob/main/big%20data/Lab_2_3_technique_handle_large_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to handle large datasets in Python with Pandas"
      ],
      "metadata": {
        "id": "vbOaO8M5Dj7r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset**: [NYC Yellow Taxi Trip Data](https://https://www.kaggle.com/datasets/elemento/nyc-yellow-taxi-trip-data)\n",
        "\n",
        "New York City (NYC) Taxi & Limousine Commission (TLC) keeps data from all its cabs, and it is freely available to download from its official website. Now, the TLC primarily keeps and manages data for 4 different types of vehicles:\n",
        "\n",
        "1. Yellow Taxi: Yellow Medallion Taxicabs: These are the famous NYC yellow taxis that provide transportation exclusively through street hails. The number of taxicabs is limited by a finite number of medallions issued by the TLC. You access this mode of transportation by standing in the street and hailing an available taxi with your hand. The pickups are not pre-arranged.\n",
        "2. Green Taxi: Street Hail Livery: The SHL program will allow livery vehicle owners to license and outfit their vehicles with green borough taxi branding, meters, credit card machines, and ultimately the right to accept street hails in addition to pre-arranged rides.\n",
        "3. For-Hire Vehicles (FHVs): FHV transportation is accessed by a pre-arrangement with a dispatcher or limo company. These FHVs are not permitted to pick up passengers via street hails, as those rides are not considered pre-arranged.\n",
        "\n",
        "**Important Points**\n",
        "\n",
        "In this dataset, we are considering only the Yellow Taxis Data, for the months of Jan 2015 & Jan-mar 2016.\n",
        "If you go over to the website of NYC TLC, and download any of the CSV files, you will find a different format of these files. This is because, the TLC regularly adds more data, alongside updating the existing one.\n",
        "One of the key changes that they have made to their data is that, instead of providing the pickup & dropoff coordinates, they have divided the NYC into regions and indexed those regions, and in the CSV files, they have provided these indices.\n",
        "Due to this reason only, I have made this dataset using the previous version of the CSV files. This dataset allows me to practice my clustering knowledge alongside my time-series knowledge.\n",
        "If you want to leave out the clustering part, then just go over to their website, and download the new CSV files."
      ],
      "metadata": {
        "id": "fFKD_j7rD0xi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Techniques to handle large datasets\n",
        "\n",
        "We will be using NYC Yellow Taxi Trip Data for the year 2016. The size of the dataset is around 1.5 GB which is good enough to explain the below techniques."
      ],
      "metadata": {
        "id": "kd0UseeWEtke"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PX7T9YklDZpO",
        "outputId": "07b064d3-e90b-410a-bfcd-7c82500b0bef"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "VendorID                   int64\n",
              "tpep_pickup_datetime      object\n",
              "tpep_dropoff_datetime     object\n",
              "passenger_count            int64\n",
              "trip_distance            float64\n",
              "pickup_longitude         float64\n",
              "pickup_latitude          float64\n",
              "RatecodeID                 int64\n",
              "store_and_fwd_flag        object\n",
              "dropoff_longitude        float64\n",
              "dropoff_latitude         float64\n",
              "payment_type               int64\n",
              "fare_amount              float64\n",
              "extra                    float64\n",
              "mta_tax                  float64\n",
              "tip_amount               float64\n",
              "tolls_amount             float64\n",
              "improvement_surcharge    float64\n",
              "total_amount             float64\n",
              "dtype: object"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('yellow_tripdata_2016-01.csv')\n",
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "weSIWKUGDZpQ",
        "outputId": "4fc4a010-07aa-4fe8-cccc-ad085d1c2f54"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.5439861416816711"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.memory_usage().sum()/(1024*1024*1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLohHuFgDZpR",
        "outputId": "b1b734a1-41e6-4d0e-ba37-b6942412b067"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "83.21279907226562"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['store_and_fwd_flag'].memory_usage()/(1024*1024)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Use efficient data types\n",
        "When you load the dataset into pandas dataframe, the default datatypes assigned to each column are not memory efficient. \n",
        "\n",
        "If we can convert these data types into memory-efficient ones we can save a lot of memory. For example, int64 can be downcast to int8 or int16 or int32 depending upon the max and min value the column holds.The below code takes of this downcasting for all numeric datatypes excluding object and DateTime types."
      ],
      "metadata": {
        "id": "z0Bg9aqEFsrE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eblPK-XCDZpT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def reduce_mem_usage(df):\n",
        "    start_mem = df.memory_usage().sum() / 1024**3\n",
        "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        \n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype('category')\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**3\n",
        "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see from the below result, the size of the dataset is drastically reduced, about 60%, after downcasting the data types of columns. In the 2nd screenshot, you can see that data types are changed to int8 or float16 or float 32."
      ],
      "metadata": {
        "id": "9iwGCuO_Hrbo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FG5mHv_-DZpU",
        "outputId": "9f345b96-73e0-410b-9d89-858a3e60c28c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Memory usage of dataframe is 1.54 MB\n",
            "Memory usage after optimization is: 0.63 MB\n",
            "Decreased by 59.3%\n"
          ]
        }
      ],
      "source": [
        "df_new = reduce_mem_usage(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What happened here actually? Let’s take a column named passenger_count as an example. It holds values 0, 1, 2, 3, 4, 5, 6, 7, 8, 9. The default data type was int64. Do you need 64 bits to store these 10 values? No. 8 bits or 1 byte is enough to hold these values. Hence it will be down-casted into int8. Similar logic goes into other numeric data types.\n",
        "\n",
        "What about “object” data type? converting object data type to a category can also save a lot of memory. For the given dataset, store_and_fwd_flag was converted to category type. \n",
        "As you can see from the below screenshot, the size of the columns reduced from 83 Mb to just 10 MB. "
      ],
      "metadata": {
        "id": "XOC8fSEgId2i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8y02-W0qDZpU",
        "outputId": "ddc4feb1-976b-4143-967b-6025142e6d64"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "VendorID                     int8\n",
              "tpep_pickup_datetime     category\n",
              "tpep_dropoff_datetime    category\n",
              "passenger_count              int8\n",
              "trip_distance             float32\n",
              "pickup_longitude          float16\n",
              "pickup_latitude           float16\n",
              "RatecodeID                   int8\n",
              "store_and_fwd_flag       category\n",
              "dropoff_longitude         float16\n",
              "dropoff_latitude          float16\n",
              "payment_type                 int8\n",
              "fare_amount               float32\n",
              "extra                     float16\n",
              "mta_tax                   float16\n",
              "tip_amount                float16\n",
              "tolls_amount              float16\n",
              "improvement_surcharge     float16\n",
              "total_amount              float32\n",
              "dtype: object"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_new.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['store_and_fwd_flag'].memory_usage()/(1024*1024)"
      ],
      "metadata": {
        "id": "amdKdn17ILv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ej373hAUDZpV",
        "outputId": "3219a4f8-a3fd-47c0-a1f8-f94f949ba059"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10.401758193969727"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_new['store_and_fwd_flag'].memory_usage()/(1024*1024)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Remove unwanted columns\n",
        "Sometimes you don’t need all the columns/features for your analysis. In such situations, you don’t have to load the dataset into pandas dataframe and then delete it. \n",
        "\n",
        "Instead, you can exclude the columns while loading the dataframe. This method along with the efficient data type can save reduce the size of the dataframe significantly. "
      ],
      "metadata": {
        "id": "J29xDfMHIoIN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOVn5DRSDZpV"
      },
      "source": [
        "## 3. Chunking\n",
        "Do you know Pandas read_csv, read_excel, etc. have ***chunksize*** parameter that can be used to read larger than the memory datasets?\n",
        "\n",
        "When you use ***chunksize*** parameter, it returns an iterable object of the type TextFileReader. Next, as with any other iterable, you can iterate over this object until data is exhausted.\n",
        "Refer to our article here to understand more about the iterables and iterators. \n",
        "\n",
        "In the below example, we are using chucksize of 100,000. What this means is that Pandas reads 100,000 each time and returns iterable called reader. Now you can perform any operation on this reader object. Once the processing on this object is done, Pandas reads next 100,000 records and the process continues until all the records are processed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZHNpm_ZDZpV"
      },
      "outputs": [],
      "source": [
        "fare_amount_max = []\n",
        "\n",
        "for reader in pd.read_csv('yellow_tripdata_2016-01.csv', chunksize=100000):\n",
        "    # do any processing on reader\n",
        "    fare_amount_max.append(reader['fare_amount'].max())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52nxKI6sDZpW",
        "outputId": "caf0e9f4-b6b2-4b2e-fcdd-c7ce7a07a003"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "111270.85"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max(fare_amount_max)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Note that this method of using chunksize is useful only when the operation you are performing doesn’t require coordination between the chunks. \n",
        "\n"
      ],
      "metadata": {
        "id": "GOJGdhBJI9iz"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}